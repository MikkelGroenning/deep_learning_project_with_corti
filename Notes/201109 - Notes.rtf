{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fswiss\fcharset0 Helvetica-Oblique;
}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs48 \cf0 Notes November 9th\

\f1\b0\fs28 - There is a high probability of the models failing as no one has done anomaly detection with succes on text yet.\
- Try to encode/decode a VAE with a RNN\
- A simple baseline is nice\
- We can start with e.g. TF-IDF to show the difference between two datasets.\
- We can then try to train the VLAE to see if that also can find the difference or not. Either outcome is interesting in that sense.\
- The sparsity of words can maybe be a factor in it working.\
- We can maybe make it character-based instead of word-based in the vocabulary. An in between solution can also be used like using n-grams. Word embeddings are also an alternative. It is a balance between having \'93out of vocabulary\'94 and learning 
\f2\i what words are
\f1\i0 . \
- LDA could be interesting to look at as a baseline.\
- Det er alts\'e5 kun p\'e5 DTU, at man kan m\'f8de op med sokker i et par Birkenstock i november.}