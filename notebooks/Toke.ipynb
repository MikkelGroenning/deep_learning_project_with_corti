{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('deep-learning-vlae': conda)",
   "metadata": {
    "interpreter": {
     "hash": "274bd8b50ba379e2de66ee05cc732351610b06b0d2e9011a22cb8485059b79dd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/Interim/hydrated_Tweet200316.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "df['CleanTweet'] = [re.sub(\"[^a-zA-Z0-9\\s,.-_´&%'\\\":€$£!?']\", '',  re.sub(' http\\S+', '', re.sub('\\s',' ', tw))).replace(u'\\xa0', u' ').lower() if isinstance(tw, str) else '' for tw in df.tweet]\n",
    "\n",
    "# Get bigrams\n",
    "bigrams_tweets = [list(nltk.bigrams(tw)) for tw in df['CleanTweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Processed/Bigram_tweets.pickle', 'wb') as handle:\n",
    "    pickle.dump(bigrams_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(''.join(set(''.join(df['CleanTweet'])))) # Assume all characters are in tweets from df1\n",
    "\n",
    "unique_bigrams = [x+y for x in unique_chars for y in unique_chars]\n",
    "\n",
    "bigram_mapper = dict(zip(unique_bigrams, range(len(unique_bigrams))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Vector.fit_transform(df['CleanTweet'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, verbose = 2, n_jobs = 3)\n",
    "lda.fit(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(X[:10000]).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.bincount(topics)\n",
    "ii = np.nonzero(y)[0]\n",
    "dict(zip(ii,y[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.score(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(X[:2000].todense(), num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/embedding.pickle\", \"rb\") as f:\n",
    "    embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([40, 10, 300])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from src.models.rvae import RecurrentVariationalAutoencoder\n",
    "from src.models.rae import RecurrentAutoencoder\n",
    "from src.models.rvae_words import RVAEWords\n",
    "from src.models.common import *\n",
    "from src.data.words import *\n",
    "from src.data.characters import *\n",
    "\n",
    "import torch\n",
    "\n",
    "rvae_w, rvae_w_tinfo = get_trained_model(RVAEWords, training_info=True)\n",
    "\n",
    "data = torch.load(\"../data/processed/200316_embedding.pkl\")\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "ds = TwitterDataWords(data[-10:])\n",
    "loader = get_loader(ds, batch_size=num_samples)\n",
    "\n",
    "x = next(iter(loader))\n",
    "\n",
    "output = rvae_w(x)\n",
    "\n",
    "# Sample from observation model and pack, then pad\n",
    "sample_packed = PackedSequence(output['px'].sample(), x.batch_sizes)\n",
    "sample_padded, _ = pad_packed_sequence(sample_packed)\n",
    "\n",
    "target_padded, _ = pad_packed_sequence(x)\n",
    "\n",
    "# word x batch x embedding_dim\n",
    "print(sample_padded.shape)\n",
    "\n",
    "for i in range(num_samples):\n",
    "\n",
    "    decoded_tweet = sample_padded[:, i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0.5129075773060322, 0.35068803895264866, 0.3410208935188737)\n",
      "(0.5270840534142085, 0.3685850002935955, 0.3374059333518441)\n",
      "(0.543899441404002, 0.3568235156791551, 0.4034492993815841)\n",
      "(0.5090602698425452, 0.3469504117965698, 0.3434752716877078)\n",
      "(0.5416954826740992, 0.3382833855492728, 0.32726740910620783)\n",
      "(0.5203982701427058, 0.3447999915010051, 0.3455739531507516)\n",
      "(0.5089935730485355, 0.3343326493221171, 0.37804772814587034)\n",
      "(0.5297945118867434, 0.3161477589836487, 0.3923036544400154)\n",
      "(0.550508975982666, 0.38519537895917894, 0.42148862313303737)\n",
      "(0.6292392100606646, 0.4653415467057909, 0.4580013376048733)\n"
     ]
    }
   ],
   "source": [
    "cse = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    print(actual_decode_similarity(sample_padded[:, i, :], target_padded[:, i, :], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}