{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/Interim/hydrated_Tweet200316.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "df['CleanTweet'] = [re.sub(\"[^a-zA-Z0-9\\s,.-_´&%'\\\":€$£!?']\", '',  re.sub(' http\\S+', '', re.sub('\\s',' ', tw))).replace(u'\\xa0', u' ').lower() if isinstance(tw, str) else '' for tw in df.tweet]\n",
    "\n",
    "# Get bigrams\n",
    "bigrams_tweets = [list(nltk.bigrams(tw)) for tw in df['CleanTweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Processed/Bigram_tweets.pickle', 'wb') as handle:\n",
    "    pickle.dump(bigrams_tweets, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(''.join(set(''.join(df['CleanTweet'])))) # Assume all characters are in tweets from df1\n",
    "\n",
    "unique_bigrams = [x+y for x in unique_chars for y in unique_chars]\n",
    "\n",
    "bigram_mapper = dict(zip(unique_bigrams, range(len(unique_bigrams))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df['CleanTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(487972, 3481)"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 1 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 2 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 3 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 4 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 5 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 6 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 7 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 8 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 9 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 10 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, n_jobs=3, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, verbose = 2, n_jobs = 3)\n",
    "lda.fit(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "topics = lda.transform(X[:10000]).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 424, 1: 891, 2: 4264, 3: 2179, 4: 2242}"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "y = np.bincount(topics)\n",
    "ii = np.nonzero(y)[0]\n",
    "dict(zip(ii,y[ii]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.16133972, 0.13859811, 0.69638492, 0.00183662, 0.00184063],\n",
       "       [0.58946017, 0.01020377, 0.01020963, 0.01012878, 0.37999766],\n",
       "       [0.00727604, 0.0072855 , 0.97082498, 0.00733381, 0.00727967],\n",
       "       ...,\n",
       "       [0.04489188, 0.00181722, 0.34178056, 0.39818723, 0.21332312],\n",
       "       [0.12547465, 0.37938139, 0.12883567, 0.00448361, 0.36182468],\n",
       "       [0.00400016, 0.11669848, 0.00396122, 0.18567844, 0.6896617 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "lda.transform(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-4636467.336107308"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "lda.score(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerTfidf = TfidfVectorizer(vocabulary = bigram_mapper, ngram_range = (2,2), analyzer = \"char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizerTfidf.fit_transform(df['CleanTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 1 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 2 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 3 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 4 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 5 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 6 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 7 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 8 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 9 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "iteration: 10 of max_iter: 10\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, n_jobs=3, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, verbose = 2, n_jobs = 3)\n",
    "lda.fit(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-455997.85389304534"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "lda.score(X[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 12.6 MB/s \n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 10.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107095 sha256=0fc7f64dbb397e51a7e1259e5732567068a88d978f385569b46af4a207ef1a78\n",
      "  Stored in directory: /Users/toke/Library/Caches/pip/wheels/83/a6/12/bf3c1a667bde4251be5b7a3368b2d604c9af2105b5c1cb1870\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Data/Processed/200316.pkl', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "largescale testing for covid19 and wide availability of testing on demand would seem a nobrainer in early detect stories on covid_19  smart. more of this, please. not going to wear a bra for a month.   socialisolation coronavirus a covid19 coronavirus update from concerned physicians via @kevinmd start thinking about holy week and easter... white house urges americans not to hoard as coronavirus death toll hits 62 do any of you get smithsonian channel? search tonight 'america's hidden storiespandemic 1918' rod i love you bro but i promise i thought this was you   top brazilian fintwit influencers march 413, the coronavirus selloff: tier 8 cont. @brolo_rodrigo cannot be opened for training!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0c533749a5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CleanTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m     \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: largescale testing for covid19 and wide availability of testing on demand would seem a nobrainer in early detect stories on covid_19  smart. more of this, please. not going to wear a bra for a month.   socialisolation coronavirus a covid19 coronavirus update from concerned physicians via @kevinmd start thinking about holy week and easter... white house urges americans not to hoard as coronavirus death toll hits 62 do any of you get smithsonian channel? search tonight 'america's hidden storiespandemic 1918' rod i love you bro but i promise i thought this was you   top brazilian fintwit influencers march 413, the coronavirus selloff: tier 8 cont. @brolo_rodrigo cannot be opened for training!"
     ]
    }
   ],
   "source": [
    "fasttext.train_unsupervised(\" \".join(df['CleanTweet'][:10]),  model='cbow', minCount = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-537cb0596b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CleanTweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "text = \" \".join(df['CleanTweet'])\n",
    "f = open('test', 'w')\n",
    "f.write(text.encode('utf8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = api.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('donald', 0.6552975177764893),\n",
       " ('clinton', 0.5108779668807983),\n",
       " ('biden', 0.5093722939491272),\n",
       " ('romney', 0.5069374442100525),\n",
       " ('warren', 0.4898340106010437),\n",
       " ('birther', 0.4887540936470032),\n",
       " ('judd', 0.47777795791625977),\n",
       " ('bloomberg', 0.4759959876537323),\n",
       " ('ivanka', 0.4735480546951294),\n",
       " ('reid', 0.4727659821510315)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "model2 = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('trumps', 0.7198434472084045),\n",
       " ('trumping', 0.5805853009223938),\n",
       " ('supersede', 0.5600423216819763),\n",
       " ('trumped', 0.5497317910194397),\n",
       " ('supercede', 0.5309919118881226),\n",
       " ('prevail', 0.487763375043869),\n",
       " ('outweigh', 0.4785327911376953),\n",
       " ('trample', 0.4714253544807434),\n",
       " ('overshadow', 0.47011539340019226),\n",
       " ('dictate', 0.46754559874534607)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model2.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.29550153\n0.29352903\n0.13284826\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('white','criminal'))\n",
    "print(model.similarity('black','criminal'))\n",
    "print(model.similarity('nigger','criminal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.04107806\n0.08380792\n0.15167528\n"
     ]
    }
   ],
   "source": [
    "print(model2.similarity('white','criminal'))\n",
    "print(model2.similarity('black','criminal'))\n",
    "print(model2.similarity('nigger','criminal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word 'electiom' not in vocabulary\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d45b2a267cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electiom\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/social_graphs_project/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'electiom' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"electiom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.1476  ,  0.24372 , -0.44205 , -0.36565 , -0.012016, -0.31824 ,\n",
       "       -0.9102  ,  0.55994 , -0.20795 , -0.41888 , -0.48952 , -0.28316 ,\n",
       "       -0.5951  , -0.18151 ,  0.61523 , -0.28698 , -0.19773 , -0.023651,\n",
       "        0.0575  ,  0.5552  ,  0.051025, -0.11355 ,  0.42244 ,  0.2664  ,\n",
       "       -0.23722 , -1.5907  ,  0.23997 , -0.43092 ,  0.031012, -0.31035 ,\n",
       "       -0.20378 , -0.14473 , -0.080509, -0.088075,  0.11065 ,  0.48304 ,\n",
       "        0.42847 ,  0.48379 , -0.11828 ,  0.17017 ,  1.0133  ,  0.36426 ,\n",
       "       -0.36842 , -0.24873 ,  0.12573 , -0.10963 ,  0.23704 ,  0.1857  ,\n",
       "       -0.21525 ,  0.21762 ,  0.0513  , -0.33661 ,  0.40283 , -0.19627 ,\n",
       "       -0.10017 , -0.057906,  0.12301 , -0.16951 , -0.10937 , -0.48194 ,\n",
       "        0.026439, -0.53014 , -0.56541 , -0.54095 ,  0.5213  , -0.38697 ,\n",
       "       -0.017976, -0.66467 ,  0.20409 , -0.72905 ,  0.26052 , -0.43418 ,\n",
       "       -0.21438 ,  0.41414 ,  0.34679 ,  0.28401 , -1.0981  ,  0.042206,\n",
       "        0.13505 , -0.13117 ,  0.43902 , -0.21741 ,  0.70798 , -0.27818 ,\n",
       "       -0.093888,  0.5283  , -0.11772 , -0.26154 , -0.7071  ,  0.40644 ,\n",
       "       -0.25205 , -0.22767 , -0.50237 , -0.71853 , -0.10534 , -0.87628 ,\n",
       "       -0.99407 , -0.36037 , -0.72453 ,  0.061738, -0.75572 ,  0.59447 ,\n",
       "        0.33932 ,  0.2516  , -0.3038  , -0.71905 ,  0.34481 ,  0.084548,\n",
       "       -0.35069 ,  0.66588 ,  0.1427  ,  0.47848 ,  0.17346 , -0.85643 ,\n",
       "        0.39365 , -0.34047 , -1.0099  ,  0.1784  , -0.24278 ,  0.72077 ,\n",
       "       -0.36501 , -1.2969  ,  0.46591 ,  0.12237 , -0.47766 ,  0.86689 ,\n",
       "        0.28969 , -0.13281 ,  0.35254 , -0.13191 , -0.20579 , -0.15623 ,\n",
       "        0.14914 , -0.14436 ,  0.40585 , -0.24738 ,  0.34473 , -0.094837,\n",
       "        0.28816 ,  0.56064 , -0.5455  ,  0.14041 ,  0.61222 ,  0.36394 ,\n",
       "        0.43346 , -0.63635 ,  0.53058 ,  0.62308 ,  0.12965 ,  0.53623 ,\n",
       "       -0.091765,  0.27143 , -1.8218  , -0.26334 , -0.1353  ,  0.53952 ,\n",
       "       -0.3537  , -0.26903 , -0.31987 ,  0.21939 , -0.19285 , -0.06661 ,\n",
       "       -0.01329 ,  0.021403, -0.15041 ,  0.30505 , -0.41342 ,  0.18189 ,\n",
       "        0.48047 , -0.29319 , -0.4615  ,  0.31862 ,  0.27881 ,  0.24426 ,\n",
       "       -0.18951 , -0.40936 ,  0.32395 , -0.047543,  0.071915, -0.12186 ,\n",
       "       -0.2866  , -0.32859 ,  0.61733 , -0.059782,  0.23561 ,  0.57813 ,\n",
       "        0.06896 , -0.64859 , -0.55016 ,  0.26546 ,  0.59342 ,  0.27756 ,\n",
       "        0.34047 ,  0.39354 ,  0.14856 , -0.54959 , -0.044447,  0.58545 ,\n",
       "        0.38675 ,  0.022943], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "model['corona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}