{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.2 64-bit ('deep-learning-vlae': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d73276c40580c1b791c77282cdf03a43473a7aa7e67de6009dcfaf9e747a87f6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.rvae import get_model\n",
    "from src.models.common import *\n",
    "from src.data.data_loader import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../../data/interim/hydrated/200316.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "dataset_test = TwitterDataset(data.iloc[int(len(data)*0.8):int(len(data)*0.8)+10, :].copy())\n",
    "\n",
    "batch_size = 10\n",
    "test_loader = get_loader(dataset_test, batch_size)\n",
    "\n",
    "x = next(iter(test_loader))\n",
    "\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, PackedSequence\n",
    "x_rec = PackedSequence(output['px'].sample(), x.batch_sizes)\n",
    "x_rec_padded, _ = pad_packed_sequence(x_rec)\n",
    "original_padded, _ = pad_packed_sequence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nda ... and honestly, i don't believe a word he says #coronavirusoutbreak #coronapocolypse #covid19E\n&tao tcneeril  eyeiteiieenutsni i#sbtembydhsse e gl# o#oonairsrucsebolcocalphtvdpu vaaocwad v19d9919\n\ncost UofU having to use large amounts of data to access class materials.U #academictwitter @Eaaaaaaa\nole u reritimttw enh stanottg ivgi#ficsso i  .llnrtnirmnvpia yiiuad.iv#iimndlircessyottyE@E@Eaaaaaaa\n\n care professionals #covid19 #thankyouforyourserviceirishhealthcareEaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\ndstew m 'wliher!ec ##oiid19  wpeiswh#to ettata#ltsueat crtinixrstEeeaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\nates via link belowEaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n a e dnlismmninoihttaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n"
     ]
    }
   ],
   "source": [
    "for original, output in zip(original_padded.transpose(1,0), x_rec_padded.transpose(1, 0)):\n",
    "    print(\"\".join([alphabet[i] for i in original][-100:]))\n",
    "    print(\"\".join([alphabet[i] for i in output][-100:]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 8,  3, 19,  ...,  0, 19, 19],\n",
       "         [15, 54, 54,  ...,  8, 20, 19],\n",
       "         [54,  2,  4,  ..., 17, 17, 17],\n",
       "         ...,\n",
       "         [53, 53,  0,  ...,  0,  0,  0],\n",
       "         [53,  4,  0,  ...,  0,  0,  0],\n",
       "         [18, 53,  0,  ...,  0,  0,  0]]),\n",
       " tensor([270, 270, 219, 150, 149, 126, 126, 117,  62,  37]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x_rec_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}